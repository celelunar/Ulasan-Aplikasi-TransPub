{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Install and Import Libraries"
      ],
      "metadata": {
        "id": "rJIAMNxvhK1L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this first, then restart runtime\n",
        "!pip install bertopic"
      ],
      "metadata": {
        "collapsed": true,
        "id": "xqf05MYThRCR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "id": "LvHaBy2ou09W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Sastrawi"
      ],
      "metadata": {
        "id": "3Uz8cPeDyAJF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import math\n",
        "import random\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from bertopic.vectorizers import ClassTfidfTransformer\n",
        "\n",
        "from bertopic import BERTopic\n",
        "from bertopic.representation import KeyBERTInspired, MaximalMarginalRelevance\n",
        "\n",
        "from umap import UMAP\n",
        "from hdbscan import HDBSCAN\n",
        "\n",
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "from gensim.corpora.dictionary import Dictionary\n",
        "\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "import torch\n",
        "from google.colab import drive, files\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
      ],
      "metadata": {
        "id": "8kDxe1raheyj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Set Up Environment"
      ],
      "metadata": {
        "id": "D9fbdNhTeDO1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1. Set Seed for Reproducibility"
      ],
      "metadata": {
        "id": "8IwhDNRAeGxl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SEED = 42"
      ],
      "metadata": {
        "id": "0amkzkJxRmUh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  torch.cuda.manual_seed_all(SEED)"
      ],
      "metadata": {
        "id": "A4ItpUeyRnbe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2. Set GPU"
      ],
      "metadata": {
        "id": "UiI_llSoeJbq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {DEVICE}\")"
      ],
      "metadata": {
        "id": "PpTe08YOuWfQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3. Manage Google Drive"
      ],
      "metadata": {
        "id": "_FN7QkfteNLO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3.1. Mount Google Drive"
      ],
      "metadata": {
        "id": "-gDTGyJ0eSgW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "yTMMBg5tw1Jz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3.2. Set Folder & File Path"
      ],
      "metadata": {
        "id": "4aFWaYsgenQo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset & Stopwords File\n",
        "EMOJI_WORDS_FILE_PATH = '/content/drive/My Drive/Thesis_NS/Dataset/emoji_words.csv'\n",
        "NO_EMOJI_FILE_PATH = '/content/drive/My Drive/Thesis_NS/Dataset/no_emoji.csv'\n",
        "STOPWORDS_PATH = '/content/drive/My Drive/Thesis_NS/tala-stopwords-indonesia.txt'\n",
        "\n",
        "# Results\n",
        "SAVE_ROOT = \"/content/drive/MyDrive/Thesis_NS/Models/TM_Check\"\n",
        "EMBED_SAVE_ROOT = \"/content/drive/MyDrive/Thesis_NS/Models/TM_Tuned_2\""
      ],
      "metadata": {
        "id": "kZlzykgb7Ph1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.4. Load Dataset"
      ],
      "metadata": {
        "id": "MAoDVYHUo0IL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_emoji_words = pd.read_csv(EMOJI_WORDS_FILE_PATH)\n",
        "df_no_emoji = pd.read_csv(NO_EMOJI_FILE_PATH)"
      ],
      "metadata": {
        "id": "9zFJgZPopCF1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_emoji_words = df_emoji_words.dropna(axis=1, how='all')\n",
        "df_no_emoji = df_no_emoji.dropna(axis=1, how='all')"
      ],
      "metadata": {
        "id": "Y7dMXIRBKO-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_emoji_words = df_emoji_words.dropna(axis=0, how='all')\n",
        "df_no_emoji = df_no_emoji.dropna(axis=0, how='all')"
      ],
      "metadata": {
        "id": "FFiPnGgRKUMc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_emoji_words = df_emoji_words.reset_index(drop=True)\n",
        "df_no_emoji = df_no_emoji.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "iOd-o5ZgKV7S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Emoji Dataset Rows: {len(df_emoji_words)}\")\n",
        "print(f\"No Emoji Dataset Rows: {len(df_no_emoji)}\")\n",
        "\n",
        "print(\"\\n--- Last 2 rows of Emoji Dataset ---\")\n",
        "print(df_emoji_words.tail(2))\n",
        "\n",
        "print(\"\\n--- Last 2 rows of No Emoji Dataset ---\")\n",
        "print(df_no_emoji.tail(2))"
      ],
      "metadata": {
        "collapsed": true,
        "id": "6IWWJvsOHSBT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert len(df_emoji_words) == len(df_no_emoji), \"Datasets must have the same number of rows!\""
      ],
      "metadata": {
        "id": "wo-K74xDKMp-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiments = [\"positive\", \"negative\"]"
      ],
      "metadata": {
        "id": "gxb90koMrciq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Topic Modelling using BERTopic"
      ],
      "metadata": {
        "id": "5YgbVd6Ex5B8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1. Set Up Components"
      ],
      "metadata": {
        "id": "llwC--RYuBDu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1.1. Embedding Models\n",
        "Convert text data to numerical vectors"
      ],
      "metadata": {
        "id": "pmgJh-IFpGt-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embed_allindo = SentenceTransformer('LazarusNLP/all-indo-e5-small-v4', device = DEVICE)"
      ],
      "metadata": {
        "id": "tes1tMRIpQIG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1.2. Set Vectorizer Model\n",
        "Convert embeddings to feature matrix of word's importance and co-occurence within the documents"
      ],
      "metadata": {
        "id": "biYX5LtapTEJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_stopwords(filepath):\n",
        "  stopwords = set()\n",
        "\n",
        "  # Add from Tala\n",
        "  if not os.path.exists(filepath):\n",
        "      print(f\"❌ Error: Stopwords file not found at {filepath}\")\n",
        "      return []\n",
        "\n",
        "  with open(filepath, 'r', encoding='utf-8') as f:\n",
        "      tala_stopwords = [line.strip() for line in f if line.strip()]\n",
        "      stopwords.update(tala_stopwords)\n",
        "\n",
        "  # Add from Sastrawi\n",
        "  factory = StopWordRemoverFactory()\n",
        "  sastrawi_stopwords = factory.get_stop_words()\n",
        "  stopwords.update(sastrawi_stopwords)\n",
        "\n",
        "  # List of stopwords found after manually checking the words frequency list AFTER stopwords removal (ADDITIONAL)\n",
        "  manual_add = ['nya', 'ya', 'ap', 'ok', 'sih', 'deh', 'tau', 'gue', 'kak', 'eh', 'gua', 'tuh', 'lu', 'the', 'by', 'hadeh', 'ku', 'jis', 'an', 'dah', 'mah', 'loh', 'iya', 'you', 'ayo', 'wow', 'jos', 'sip', 'aduh', 'anjir', 'and', 'apatu', 'ah', 'si', 'duh', 'mbak', 'kah', 'amin', 'this', 'mu', 'baiknya', 'berkali', 'kali', 'kurangnya', 'mata', 'olah', 'sekurang', 'setidak', 'tama', 'tidaknya', 'banget', 'pas', 'kayak', 'oke']\n",
        "  stopwords.update(manual_add)\n",
        "\n",
        "  return list(stopwords)"
      ],
      "metadata": {
        "id": "5z7n5Y3vpcNl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords = load_stopwords(STOPWORDS_PATH)"
      ],
      "metadata": {
        "id": "DV9QtNQSqXh5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer_model = CountVectorizer(ngram_range=(1, 2), stop_words=stopwords)"
      ],
      "metadata": {
        "id": "d9HKxDPzqYav"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1.3. Class TF-IDF Model\n",
        "Weighting terms based on their relevance to specific topics rather than overall corpus"
      ],
      "metadata": {
        "id": "37R4ke-Tpcbu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True)"
      ],
      "metadata": {
        "id": "ur9F90HIplHQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2. Helper Functions"
      ],
      "metadata": {
        "id": "Z-gRLbSS0M8d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2.3. Calculate Model Performance"
      ],
      "metadata": {
        "id": "5nS-zAFOkSvH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_topic_coherence(topic_model, docs, top_n=10):\n",
        "    print(\"Calculating topic coherence with Gensim...\")\n",
        "\n",
        "    topics = topic_model.get_topics()\n",
        "    top_n_words = [\n",
        "        [w for w, _ in words[:top_n]]\n",
        "        for topic_id, words in topics.items() if topic_id != -1 and len(words) > 0\n",
        "    ]\n",
        "\n",
        "    texts = [doc.split() for doc in docs]\n",
        "    dictionary = Dictionary(texts)\n",
        "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
        "\n",
        "    coherence_score = []\n",
        "    try:\n",
        "        coherence_score = CoherenceModel(\n",
        "            topics=top_n_words, texts=texts, dictionary=dictionary, coherence=\"c_v\"\n",
        "        ).get_coherence()\n",
        "    except:\n",
        "        coherence_score = np.nan\n",
        "    return coherence_score"
      ],
      "metadata": {
        "id": "4VITbC77rxPJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_topic_diversity(topic_model, top_n=10):\n",
        "    print(\"Calculating topic diversity...\")\n",
        "    topics = topic_model.get_topics()\n",
        "\n",
        "    # filter out outlier -1\n",
        "    topic_ids = [tid for tid in topics.keys() if tid != -1]\n",
        "    topic_wordsets = []\n",
        "    for tid in topic_ids:\n",
        "        words = [w for w, _ in topic_model.get_topic(tid)[:top_n]]\n",
        "        topic_wordsets.append(set(words))\n",
        "\n",
        "    # pairwise Jaccard\n",
        "    overlaps = []\n",
        "    for i in range(len(topic_wordsets)):\n",
        "        for j in range(i+1, len(topic_wordsets)):\n",
        "            a, b = topic_wordsets[i], topic_wordsets[j]\n",
        "            if len(a.union(b)) == 0:\n",
        "                continue\n",
        "            overlaps.append(len(a.intersection(b)) / len(a.union(b)))\n",
        "    diversity_score = 1 - (sum(overlaps) / len(overlaps)) if overlaps else 0\n",
        "    print(f\"Topic Diversity: {diversity_score}\")\n",
        "\n",
        "    return diversity_score"
      ],
      "metadata": {
        "id": "i0jLb0aor1jd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2.4. Generate Topics and Visualizations"
      ],
      "metadata": {
        "id": "HoGhC2AnsBCD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_wordcloud(topic_model, model_folder, N_COLS=3, top_n=30):\n",
        "    print(\"Generating wordcloud for all topics...\")\n",
        "\n",
        "    topic_info = topic_model.get_topic_info()\n",
        "    topics_to_plot = [t for t in topic_info[\"Topic\"].tolist() if t != -1]\n",
        "\n",
        "    n_topics = len(topics_to_plot)\n",
        "    if n_topics == 0:\n",
        "        print(\"No topics found (excluding -1).\")\n",
        "        return\n",
        "\n",
        "    N_ROWS = math.ceil(n_topics / N_COLS)\n",
        "    fig, axes = plt.subplots(N_ROWS, N_COLS, figsize=(N_COLS * 6, N_ROWS * 4), constrained_layout=True)\n",
        "    axes = axes.flatten() if hasattr(axes, \"__iter__\") else [axes]\n",
        "\n",
        "    for i, topic_id in enumerate(topics_to_plot):\n",
        "        topic_words = [word for word, _ in topic_model.get_topic(topic_id)][:top_n]\n",
        "        text_for_wordcloud = \" \".join(topic_words)\n",
        "        wc = WordCloud(background_color=\"white\", collocations=False).generate(text_for_wordcloud)\n",
        "        ax = axes[i]\n",
        "        ax.imshow(wc, interpolation=\"bilinear\")\n",
        "        ax.axis(\"off\")\n",
        "        name_series = topic_info.loc[topic_info[\"Topic\"] == topic_id, \"Name\"]\n",
        "        topic_name = name_series.values[0] if len(name_series) > 0 else \"\"\n",
        "        ax.set_title(f\"Topic {topic_id}: {topic_name}\", fontsize=12)\n",
        "\n",
        "    for j in range(n_topics, N_ROWS * N_COLS):\n",
        "        fig.delaxes(axes[j])\n",
        "\n",
        "    model_folder = Path(model_folder)\n",
        "    model_folder.mkdir(parents=True, exist_ok=True)\n",
        "    wordcloud_path = model_folder / \"wordcloud.png\"\n",
        "    fig.savefig(wordcloud_path, dpi=300, bbox_inches='tight')\n",
        "    plt.close(fig)\n",
        "\n",
        "    print(f\"✅ WordCloud saved to {wordcloud_path}\")"
      ],
      "metadata": {
        "id": "f8ykSjP7sFqe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_topics_to_txt(topic_model, model_folder, top_n=20):\n",
        "    print(\"Saving topic words to text file...\")\n",
        "\n",
        "    topics = topic_model.get_topics()\n",
        "    lines = []\n",
        "\n",
        "    for topic_id, topic_words in topics.items():\n",
        "        words = [f\"{word}: {weight:.4f}\" for word, weight in topic_words[:top_n]]\n",
        "        lines.append(f\"Topic {topic_id}: {', '.join(words)}\\n\")\n",
        "    model_folder = Path(model_folder)\n",
        "    model_folder.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    with open(model_folder / \"topics.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "        f.writelines(lines)\n",
        "\n",
        "    print(f\"✅ Topics saved to {model_folder / 'topics.txt'}\")"
      ],
      "metadata": {
        "id": "M3ndwNxZsGvO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build Model"
      ],
      "metadata": {
        "id": "wLn_wi-W0koO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def merge_params(best_params, override_params):\n",
        "    merged = best_params.copy()\n",
        "    merged.update(override_params)\n",
        "    return merged"
      ],
      "metadata": {
        "id": "oYMmcoS30ra-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_bertopic_from_params(params, embedding_model, vectorizer_model, ctfidf_model, nr_topics, seed=SEED):\n",
        "    umap_model = UMAP(\n",
        "        n_neighbors=params[\"n_neighbors\"],\n",
        "        n_components=params[\"n_components\"],\n",
        "        min_dist=0.0,\n",
        "        metric=\"cosine\",\n",
        "        random_state=seed\n",
        "    )\n",
        "\n",
        "    hdbscan_model = HDBSCAN(\n",
        "        min_cluster_size=params[\"min_cluster_size\"],\n",
        "        min_samples=params[\"min_samples\"],\n",
        "        metric=\"euclidean\",\n",
        "        cluster_selection_method=\"eom\",\n",
        "        prediction_data=True\n",
        "    )\n",
        "\n",
        "    topic_model = BERTopic(\n",
        "        embedding_model=embedding_model,\n",
        "        vectorizer_model=vectorizer_model,\n",
        "        ctfidf_model=ctfidf_model,\n",
        "        umap_model=umap_model,\n",
        "        hdbscan_model=hdbscan_model,\n",
        "        min_topic_size=params[\"min_topic_size\"],\n",
        "        nr_topics=nr_topics,\n",
        "        calculate_probabilities=False,\n",
        "        verbose=False\n",
        "    )\n",
        "\n",
        "    return topic_model"
      ],
      "metadata": {
        "id": "22D7vr-X0n2-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_manual_config(config_id, params, docs_clean, embeddings, embedding_model, vectorizer_model, ctfidf_model, save_root, nr_topics):\n",
        "    print(f\"\\nRunning Check: {config_id}\")\n",
        "    print(\"Params:\", params)\n",
        "\n",
        "    topic_model = build_bertopic_from_params(\n",
        "        params,\n",
        "        embedding_model,\n",
        "        vectorizer_model,\n",
        "        ctfidf_model,\n",
        "        nr_topics\n",
        "    )\n",
        "\n",
        "    topics, probs = topic_model.fit_transform(docs_clean, embeddings)\n",
        "\n",
        "    # Metrics\n",
        "    coh = calculate_topic_coherence(topic_model, docs_clean)\n",
        "    div = calculate_topic_diversity(topic_model)\n",
        "\n",
        "    metrics = {\n",
        "        \"c_v\": coh,\n",
        "        \"diversity\": div,\n",
        "        **params\n",
        "    }\n",
        "\n",
        "    save_path = Path(save_root) / f\"manual_config_{config_id}\"\n",
        "    save_path.mkdir(parents=True, exist_ok=True)\n",
        "    hf_model_path = save_path / \"bertopic\"\n",
        "\n",
        "    topic_model.save(\n",
        "        path=str(hf_model_path),\n",
        "        serialization=\"safetensors\",\n",
        "        save_ctfidf=True,\n",
        "        save_embedding_model=\"LazarusNLP/all-indo-e5-small-v4\"\n",
        "    )\n",
        "\n",
        "    print(f\"✅ BERTopic safetensors model saved to: {hf_model_path}\")\n",
        "\n",
        "    rep_docs = topic_model.get_representative_docs()\n",
        "\n",
        "    rep_docs_serializable = {\n",
        "        str(topic_id): docs\n",
        "        for topic_id, docs in rep_docs.items()\n",
        "    }\n",
        "\n",
        "    rep_docs_path = save_path / \"representative_docs.json\"\n",
        "\n",
        "    with open(rep_docs_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(rep_docs_serializable, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    print(f\"✅ Representative docs saved to: {rep_docs_path}\")\n",
        "\n",
        "    save_topics_to_txt(topic_model, save_path)\n",
        "    create_wordcloud(topic_model, save_path)\n",
        "\n",
        "    pd.DataFrame([metrics]).to_csv(save_path / \"metrics.csv\", index=False)\n",
        "\n",
        "    with open(save_path / \"params.json\", \"w\") as f:\n",
        "        json.dump(params, f, indent=2)\n",
        "\n",
        "    print(f\"✅ Run {config_id} saved | Coherence={coh:.4f}\")\n",
        "\n",
        "    return metrics"
      ],
      "metadata": {
        "id": "YrQkkYrH0ul1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pipeline_per_sent(sentiment, df_emoji, df_clean, embed_model, vectorizer_model, ctfidf_model, optuna_best_params, new_hyperparams, save_root, nr_topics\n",
        "):\n",
        "    print(\"=\"*50)\n",
        "    print(f\"Sentiment: {sentiment}\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    mask = df_clean[\"sentiment\"] == sentiment\n",
        "    docs_clean_raw = df_clean[mask][\"cleaned_content\"].astype(str).tolist()\n",
        "    docs_emoji_raw = df_emoji[mask][\"cleaned_content\"].astype(str).tolist()\n",
        "\n",
        "    valid_idx = [i for i, d in enumerate(docs_clean_raw) if len(d.split()) > 3]\n",
        "    docs_clean = [docs_clean_raw[i] for i in valid_idx]\n",
        "    docs_emoji = [docs_emoji_raw[i] for i in valid_idx]\n",
        "\n",
        "    emb_path = Path(EMBED_SAVE_ROOT) / \"embeddings\" / f\"indo-e5_{sentiment}_emoji_context.npy\"\n",
        "    embeddings = np.load(emb_path)\n",
        "    assert len(docs_clean) == embeddings.shape[0], \\\n",
        "    f\"Docs ({len(docs_clean)}) and embeddings ({embeddings.shape[0]}) mismatch\"\n",
        "\n",
        "\n",
        "    sentiment_root = Path(save_root) / sentiment\n",
        "    sentiment_root.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    for i, override in enumerate(new_hyperparams):\n",
        "        merged_params = merge_params(optuna_best_params, override)\n",
        "        config_id = f\"{i:02d}\"\n",
        "\n",
        "        metrics = train_manual_config(\n",
        "            config_id=config_id,\n",
        "            params=merged_params,\n",
        "            docs_clean=docs_clean,\n",
        "            embeddings=embeddings,\n",
        "            embedding_model=embed_model,\n",
        "            vectorizer_model=vectorizer_model,\n",
        "            ctfidf_model=ctfidf_model,\n",
        "            save_root=sentiment_root,\n",
        "            nr_topics=nr_topics\n",
        "        )\n",
        "\n",
        "        results[config_id] = metrics\n",
        "\n",
        "    pd.DataFrame.from_dict(results, orient=\"index\") \\\n",
        "      .to_csv(sentiment_root / \"manual_hp_summary.csv\")\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "z9ZPlwRY1LNa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optuna_best_params_pos = {\n",
        "    'n_neighbors': 17, 'n_components': 9, 'min_cluster_size': 49, 'min_samples': 15, 'min_topic_size': 87\n",
        "}"
      ],
      "metadata": {
        "id": "VzIsiqZD24ZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hp_list_pos = [\n",
        "    {'n_neighbors': 5}\n",
        "    ,{'n_neighbors': 10}\n",
        "    ,{'n_neighbors': 30}\n",
        "    ,{'n_neighbors': 35}\n",
        "    ,{'n_neighbors': 40}\n",
        "    ,{'n_components': 3}\n",
        "    ,{'n_components': 15}\n",
        "    ,{'min_cluster_size': 35}\n",
        "    ,{'min_cluster_size': 20}\n",
        "    ,{'min_cluster_size': 70}\n",
        "    ,{'min_cluster_size': 80}\n",
        "    ,{'min_cluster_size': 95}\n",
        "    ,{'min_samples': 5}\n",
        "    ,{'min_samples': 20}\n",
        "    ,{'min_topic_size': 30}\n",
        "    ,{'min_topic_size': 45}\n",
        "    ,{'min_topic_size': 60}\n",
        "    ,{'min_topic_size': 75}\n",
        "    ,{'min_topic_size': 110}\n",
        "    ,{'min_topic_size': 120}\n",
        "]"
      ],
      "metadata": {
        "id": "-GJf3Wk_2Uh8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline_per_sent(\n",
        "    sentiment=\"positive\",\n",
        "    df_emoji=df_emoji_words,\n",
        "    df_clean=df_no_emoji,\n",
        "    embed_model=embed_allindo,\n",
        "    vectorizer_model=vectorizer_model,\n",
        "    ctfidf_model=ctfidf_model,\n",
        "    optuna_best_params=optuna_best_params_pos,\n",
        "    new_hyperparams=hp_list_pos,\n",
        "    save_root=SAVE_ROOT,\n",
        "    nr_topics=13\n",
        ")"
      ],
      "metadata": {
        "id": "28TdbhlJ2IOD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optuna_best_params_neg = {\n",
        "    'n_neighbors': 28, 'n_components': 9, 'min_cluster_size': 65, 'min_samples': 6, 'min_topic_size': 51\n",
        "}"
      ],
      "metadata": {
        "id": "sgt79JsI3L9F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hp_list_neg = [\n",
        "    {'n_neighbors': 20}\n",
        "    ,{'n_neighbors': 15}\n",
        "    ,{'n_neighbors': 10}\n",
        "    ,{'n_neighbors': 35}\n",
        "    ,{'n_neighbors': 40}\n",
        "    ,{'n_neighbors': 50}\n",
        "    ,{'n_components': 5}\n",
        "    ,{'n_components': 15}\n",
        "    ,{'min_cluster_size': 50}\n",
        "    ,{'min_cluster_size': 35}\n",
        "    ,{'min_cluster_size': 20}\n",
        "    ,{'min_cluster_size': 75}\n",
        "    ,{'min_cluster_size': 85}\n",
        "    ,{'min_cluster_size': 95}\n",
        "    ,{'min_samples': 3}\n",
        "    ,{'min_samples': 15}\n",
        "    ,{'min_samples': 20}\n",
        "    ,{'min_topic_size': 30}\n",
        "    ,{'min_topic_size': 45}\n",
        "    ,{'min_topic_size': 65}\n",
        "    ,{'min_topic_size': 80}\n",
        "    ,{'min_topic_size': 110}\n",
        "    ,{'min_topic_size': 120}\n",
        "]"
      ],
      "metadata": {
        "id": "ORaJuIie2X9Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline_per_sent(\n",
        "    sentiment=\"negative\",\n",
        "    df_emoji=df_emoji_words,\n",
        "    df_clean=df_no_emoji,\n",
        "    embed_model=embed_allindo,\n",
        "    vectorizer_model=vectorizer_model,\n",
        "    ctfidf_model=ctfidf_model,\n",
        "    optuna_best_params=optuna_best_params_neg,\n",
        "    new_hyperparams=hp_list_neg,\n",
        "    save_root=SAVE_ROOT,\n",
        "    nr_topics=8\n",
        ")"
      ],
      "metadata": {
        "id": "ElaZ54Un2NMW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}