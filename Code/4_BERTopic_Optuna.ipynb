{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "8IwhDNRAeGxl",
        "UiI_llSoeJbq",
        "MAoDVYHUo0IL",
        "5YgbVd6Ex5B8",
        "llwC--RYuBDu",
        "pmgJh-IFpGt-",
        "bP0blOMTiuJ3",
        "5nS-zAFOkSvH",
        "HoGhC2AnsBCD"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Install and Import Libraries"
      ],
      "metadata": {
        "id": "rJIAMNxvhK1L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this first, then restart runtime\n",
        "!pip install bertopic"
      ],
      "metadata": {
        "collapsed": true,
        "id": "xqf05MYThRCR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna"
      ],
      "metadata": {
        "id": "0PEfF4uKilap",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "collapsed": true,
        "id": "LvHaBy2ou09W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Sastrawi"
      ],
      "metadata": {
        "id": "3Uz8cPeDyAJF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import math\n",
        "import random\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import optuna\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from bertopic.vectorizers import ClassTfidfTransformer\n",
        "\n",
        "from bertopic import BERTopic\n",
        "\n",
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "from gensim.corpora.dictionary import Dictionary\n",
        "\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "import torch\n",
        "from google.colab import drive, files\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
      ],
      "metadata": {
        "id": "8kDxe1raheyj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Set Up Environment"
      ],
      "metadata": {
        "id": "D9fbdNhTeDO1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1. Set Seed for Reproducibility"
      ],
      "metadata": {
        "id": "8IwhDNRAeGxl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SEED = 42"
      ],
      "metadata": {
        "id": "0amkzkJxRmUh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  torch.cuda.manual_seed_all(SEED)"
      ],
      "metadata": {
        "id": "A4ItpUeyRnbe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2. Set GPU"
      ],
      "metadata": {
        "id": "UiI_llSoeJbq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {DEVICE}\")"
      ],
      "metadata": {
        "id": "PpTe08YOuWfQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3. Manage Google Drive"
      ],
      "metadata": {
        "id": "_FN7QkfteNLO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3.1. Mount Google Drive"
      ],
      "metadata": {
        "id": "-gDTGyJ0eSgW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "yTMMBg5tw1Jz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3.2. Set Folder & File Path"
      ],
      "metadata": {
        "id": "4aFWaYsgenQo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset & Stopwords File\n",
        "EMOJI_WORDS_FILE_PATH = '/content/drive/My Drive/Bach_Thesis/Dataset/emoji_words.csv'\n",
        "NO_EMOJI_FILE_PATH = '/content/drive/My Drive/Bach_Thesis/Dataset/no_emoji.csv'\n",
        "STOPWORDS_PATH = '/content/drive/My Drive/Bach_Thesis/tala-stopwords-indonesia.txt'\n",
        "\n",
        "# Results\n",
        "SAVE_ROOT = \"/content/drive/MyDrive/Bach_Thesis/Models/TM_Optuna\""
      ],
      "metadata": {
        "id": "kZlzykgb7Ph1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.4. Load Dataset"
      ],
      "metadata": {
        "id": "MAoDVYHUo0IL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_emoji_words = pd.read_csv(EMOJI_WORDS_FILE_PATH)\n",
        "df_no_emoji = pd.read_csv(NO_EMOJI_FILE_PATH)"
      ],
      "metadata": {
        "id": "9zFJgZPopCF1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_emoji_words = df_emoji_words.dropna(axis=1, how='all')\n",
        "df_no_emoji = df_no_emoji.dropna(axis=1, how='all')"
      ],
      "metadata": {
        "id": "Y7dMXIRBKO-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_emoji_words = df_emoji_words.dropna(axis=0, how='all')\n",
        "df_no_emoji = df_no_emoji.dropna(axis=0, how='all')"
      ],
      "metadata": {
        "id": "FFiPnGgRKUMc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_emoji_words = df_emoji_words.reset_index(drop=True)\n",
        "df_no_emoji = df_no_emoji.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "iOd-o5ZgKV7S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Emoji Dataset Rows: {len(df_emoji_words)}\")\n",
        "print(f\"No Emoji Dataset Rows: {len(df_no_emoji)}\")\n",
        "\n",
        "print(\"\\n--- Last 2 rows of Emoji Dataset ---\")\n",
        "print(df_emoji_words.tail(2))\n",
        "\n",
        "print(\"\\n--- Last 2 rows of No Emoji Dataset ---\")\n",
        "print(df_no_emoji.tail(2))"
      ],
      "metadata": {
        "collapsed": true,
        "id": "6IWWJvsOHSBT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert len(df_emoji_words) == len(df_no_emoji), \"Datasets must have the same number of rows!\""
      ],
      "metadata": {
        "id": "wo-K74xDKMp-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiments = [\"positive\", \"negative\"]"
      ],
      "metadata": {
        "id": "gxb90koMrciq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Topic Modelling using BERTopic"
      ],
      "metadata": {
        "id": "5YgbVd6Ex5B8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1. Set Up Components"
      ],
      "metadata": {
        "id": "llwC--RYuBDu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1.1. Embedding Models\n",
        "Convert text data to numerical vectors"
      ],
      "metadata": {
        "id": "pmgJh-IFpGt-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embed_allindo = SentenceTransformer('LazarusNLP/all-indo-e5-small-v4', device = DEVICE)"
      ],
      "metadata": {
        "id": "tes1tMRIpQIG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1.2. Set Vectorizer Model\n",
        "Convert embeddings to feature matrix of word's importance and co-occurence within the documents"
      ],
      "metadata": {
        "id": "biYX5LtapTEJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_stopwords(filepath):\n",
        "  stopwords = set()\n",
        "\n",
        "  # Add from Tala\n",
        "  if not os.path.exists(filepath):\n",
        "      print(f\"❌ Error: Stopwords file not found at {filepath}\")\n",
        "      return []\n",
        "\n",
        "  with open(filepath, 'r', encoding='utf-8') as f:\n",
        "      tala_stopwords = [line.strip() for line in f if line.strip()]\n",
        "      stopwords.update(tala_stopwords)\n",
        "\n",
        "  # Add from Sastrawi\n",
        "  factory = StopWordRemoverFactory()\n",
        "  sastrawi_stopwords = factory.get_stop_words()\n",
        "  stopwords.update(sastrawi_stopwords)\n",
        "\n",
        "  # List of stopwords found after manually checking the words frequency list AFTER stopwords removal (ADDITIONAL)\n",
        "  manual_add = ['nya', 'ya', 'ap', 'ok', 'sih', 'deh', 'tau', 'gue', 'kak', 'eh', 'gua', 'tuh', 'lu', 'the', 'by', 'hadeh', 'ku', 'jis', 'an', 'dah', 'mah', 'loh', 'iya', 'you', 'ayo', 'wow', 'jos', 'sip', 'aduh', 'anjir', 'and', 'apatu', 'ah', 'si', 'duh', 'mbak', 'kah', 'amin', 'this', 'mu', 'baiknya', 'berkali', 'kali', 'kurangnya', 'mata', 'olah', 'sekurang', 'setidak', 'tama', 'tidaknya', 'banget', 'pas', 'kayak', 'oke']\n",
        "  stopwords.update(manual_add)\n",
        "\n",
        "  return list(stopwords)"
      ],
      "metadata": {
        "id": "5z7n5Y3vpcNl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords = load_stopwords(STOPWORDS_PATH)"
      ],
      "metadata": {
        "id": "DV9QtNQSqXh5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer_model = CountVectorizer(ngram_range=(1, 2), stop_words=stopwords)"
      ],
      "metadata": {
        "id": "d9HKxDPzqYav"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1.3. Class TF-IDF Model\n",
        "Weighting terms based on their relevance to specific topics rather than overall corpus"
      ],
      "metadata": {
        "id": "37R4ke-Tpcbu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True)"
      ],
      "metadata": {
        "id": "ur9F90HIplHQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2. Helper Functions"
      ],
      "metadata": {
        "id": "7S28JxC4iq8S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2.1. Optuna Functions"
      ],
      "metadata": {
        "id": "bP0blOMTiuJ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from optuna.pruners import MedianPruner"
      ],
      "metadata": {
        "id": "UNsCqt-69C5d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from umap import UMAP\n",
        "from hdbscan import HDBSCAN"
      ],
      "metadata": {
        "id": "D8wcSzaGNjal"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def optuna_objective2(trial, docs, embedding_model, embeddings, vectorizer_model, ctfidf_model):\n",
        "    # UMAP search space\n",
        "    n_neighbors = trial.suggest_int(\"n_neighbors\", 10, 50)\n",
        "    n_components = trial.suggest_int(\"n_components\", 5, 10)\n",
        "\n",
        "    umap_t = UMAP(\n",
        "        n_neighbors=n_neighbors,\n",
        "        n_components=n_components,\n",
        "        min_dist=0.0,\n",
        "        metric='cosine',\n",
        "        random_state=SEED\n",
        "    )\n",
        "\n",
        "    # HDBSCAN search space\n",
        "    min_cluster_size = trial.suggest_int(\"min_cluster_size\", 10, 100)\n",
        "    min_samples = trial.suggest_int(\"min_samples\", 5, 20)\n",
        "\n",
        "    hdbscan_t = HDBSCAN(\n",
        "        min_cluster_size=min_cluster_size,\n",
        "        min_samples=min_samples,\n",
        "        metric='euclidean',\n",
        "        cluster_selection_method='eom',\n",
        "        prediction_data=True\n",
        "    )\n",
        "\n",
        "    # BERTopic search space\n",
        "    min_topic_size = trial.suggest_int(\"min_topic_size\", 20, 100)\n",
        "\n",
        "    try:\n",
        "        topic_model = BERTopic(\n",
        "            embedding_model=embedding_model,\n",
        "            vectorizer_model=vectorizer_model,\n",
        "            ctfidf_model=ctfidf_model,\n",
        "            umap_model=umap_t,\n",
        "            hdbscan_model=hdbscan_t,\n",
        "            min_topic_size=min_topic_size,\n",
        "            nr_topics=\"auto\",\n",
        "            calculate_probabilities=True,\n",
        "            verbose=False\n",
        "        )\n",
        "        topics, probs = topic_model.fit_transform(docs, embeddings)\n",
        "\n",
        "        coh = calculate_topic_coherence(topic_model, docs, top_n=10)\n",
        "        if coh is None or (isinstance(coh, float) and np.isnan(coh)):\n",
        "            return -np.inf\n",
        "\n",
        "        trial.report(coh, step=0)\n",
        "\n",
        "        if trial.should_prune():\n",
        "            raise optuna.exceptions.TrialPruned()\n",
        "\n",
        "        trial.set_user_attr(\"model\", topic_model)\\\n",
        "\n",
        "        return coh\n",
        "\n",
        "    except optuna.exceptions.TrialPruned:\n",
        "        raise\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Trial failed with error: {e}\")\n",
        "        return -np.inf"
      ],
      "metadata": {
        "id": "pNsn_oADNbLE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tuning_per_sentiment2(docs, embedding_model, embeddings, vectorizer_model, ctfidf_model, n_trials=20, study_name=None):\n",
        "    if study_name is None:\n",
        "        study_name = \"bertopic_hpo\"\n",
        "\n",
        "    pruner = MedianPruner(\n",
        "        n_startup_trials=5,\n",
        "        n_warmup_steps=0\n",
        "    )\n",
        "\n",
        "    study = optuna.create_study(\n",
        "        direction=\"maximize\",\n",
        "        study_name=study_name,\n",
        "        pruner=pruner)\n",
        "\n",
        "    study.optimize(\n",
        "        lambda trial: optuna_objective2(trial, docs, embedding_model, embeddings, vectorizer_model, ctfidf_model),\n",
        "        n_trials=n_trials\n",
        "    )\n",
        "\n",
        "    best_trial = study.best_trial\n",
        "    best_model = best_trial.user_attrs[\"model\"]\n",
        "\n",
        "    return study, best_model"
      ],
      "metadata": {
        "id": "DtYnNqJXNnHj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2.3. Calculate Model Performance"
      ],
      "metadata": {
        "id": "5nS-zAFOkSvH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_topic_coherence(topic_model, docs, top_n=10):\n",
        "    print(\"Calculating topic coherence with Gensim...\")\n",
        "\n",
        "    topics = topic_model.get_topics()\n",
        "    top_n_words = [\n",
        "        [w for w, _ in words[:top_n]]\n",
        "        for topic_id, words in topics.items() if topic_id != -1 and len(words) > 0\n",
        "    ]\n",
        "\n",
        "    texts = [doc.split() for doc in docs]\n",
        "    dictionary = Dictionary(texts)\n",
        "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
        "\n",
        "    coherence_score = []\n",
        "    try:\n",
        "        coherence_score = CoherenceModel(\n",
        "            topics=top_n_words, texts=texts, dictionary=dictionary, coherence=\"c_v\"\n",
        "        ).get_coherence()\n",
        "    except:\n",
        "        coherence_score = np.nan\n",
        "    return coherence_score"
      ],
      "metadata": {
        "id": "4VITbC77rxPJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_topic_diversity(topic_model, top_n=10):\n",
        "    print(\"Calculating topic diversity...\")\n",
        "    topics = topic_model.get_topics()\n",
        "\n",
        "    # filter out outlier -1\n",
        "    topic_ids = [tid for tid in topics.keys() if tid != -1]\n",
        "    topic_wordsets = []\n",
        "    for tid in topic_ids:\n",
        "        words = [w for w, _ in topic_model.get_topic(tid)[:top_n]]\n",
        "        topic_wordsets.append(set(words))\n",
        "\n",
        "    # pairwise Jaccard\n",
        "    overlaps = []\n",
        "    for i in range(len(topic_wordsets)):\n",
        "        for j in range(i+1, len(topic_wordsets)):\n",
        "            a, b = topic_wordsets[i], topic_wordsets[j]\n",
        "            if len(a.union(b)) == 0:\n",
        "                continue\n",
        "            overlaps.append(len(a.intersection(b)) / len(a.union(b)))\n",
        "    diversity_score = 1 - (sum(overlaps) / len(overlaps)) if overlaps else 0\n",
        "    print(f\"Topic Diversity: {diversity_score}\")\n",
        "\n",
        "    return diversity_score"
      ],
      "metadata": {
        "id": "i0jLb0aor1jd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2.4. Generate Topics and Visualizations"
      ],
      "metadata": {
        "id": "HoGhC2AnsBCD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_wordcloud(topic_model, model_folder, N_COLS=3, top_n=30):\n",
        "    print(\"Generating wordcloud for all topics...\")\n",
        "\n",
        "    topic_info = topic_model.get_topic_info()\n",
        "    topics_to_plot = [t for t in topic_info[\"Topic\"].tolist() if t != -1]\n",
        "\n",
        "    n_topics = len(topics_to_plot)\n",
        "    if n_topics == 0:\n",
        "        print(\"No topics found (excluding -1).\")\n",
        "        return\n",
        "\n",
        "    N_ROWS = math.ceil(n_topics / N_COLS)\n",
        "    fig, axes = plt.subplots(N_ROWS, N_COLS, figsize=(N_COLS * 6, N_ROWS * 4), constrained_layout=True)\n",
        "    axes = axes.flatten() if hasattr(axes, \"__iter__\") else [axes]\n",
        "\n",
        "    for i, topic_id in enumerate(topics_to_plot):\n",
        "        topic_words = [word for word, _ in topic_model.get_topic(topic_id)][:top_n]\n",
        "        text_for_wordcloud = \" \".join(topic_words)\n",
        "        wc = WordCloud(background_color=\"white\", collocations=False).generate(text_for_wordcloud)\n",
        "        ax = axes[i]\n",
        "        ax.imshow(wc, interpolation=\"bilinear\")\n",
        "        ax.axis(\"off\")\n",
        "        name_series = topic_info.loc[topic_info[\"Topic\"] == topic_id, \"Name\"]\n",
        "        topic_name = name_series.values[0] if len(name_series) > 0 else \"\"\n",
        "        ax.set_title(f\"Topic {topic_id}: {topic_name}\", fontsize=12)\n",
        "\n",
        "    for j in range(n_topics, N_ROWS * N_COLS):\n",
        "        fig.delaxes(axes[j])\n",
        "\n",
        "    model_folder = Path(model_folder)\n",
        "    model_folder.mkdir(parents=True, exist_ok=True)\n",
        "    wordcloud_path = model_folder / \"wordcloud.png\"\n",
        "    fig.savefig(wordcloud_path, dpi=300, bbox_inches='tight')\n",
        "    plt.close(fig)\n",
        "\n",
        "    print(f\"✅ WordCloud saved to {wordcloud_path}\")"
      ],
      "metadata": {
        "id": "f8ykSjP7sFqe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_topics_to_txt(topic_model, model_folder, top_n=20):\n",
        "    print(\"Saving topic words to text file...\")\n",
        "\n",
        "    topics = topic_model.get_topics()\n",
        "    lines = []\n",
        "\n",
        "    for topic_id, topic_words in topics.items():\n",
        "        words = [f\"{word}: {weight:.4f}\" for word, weight in topic_words[:top_n]]\n",
        "        lines.append(f\"Topic {topic_id}: {', '.join(words)}\\n\")\n",
        "    model_folder = Path(model_folder)\n",
        "    model_folder.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    with open(model_folder / \"topics.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "        f.writelines(lines)\n",
        "\n",
        "    print(f\"✅ Topics saved to {model_folder / 'topics.txt'}\")"
      ],
      "metadata": {
        "id": "M3ndwNxZsGvO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2.5. Full Pipeline"
      ],
      "metadata": {
        "id": "ohqnMV0ykiLf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pipeline(sentiments, df_emoji, df_clean, embed_model, vectorizer_model, ctfidf_model, base_path, config_name, n_trials_per_sentiment=20):\n",
        "    results = {}\n",
        "\n",
        "    for sentiment in sentiments:\n",
        "        print(\"=\"*40)\n",
        "        print(f\"Running tuning + training for sentiment: {sentiment}\")\n",
        "        print(\"=\"*40)\n",
        "\n",
        "        mask = (df_clean[\"sentiment\"] == sentiment)\n",
        "\n",
        "        docs_clean_raw = df_clean[mask][\"cleaned_content\"].astype(str).tolist()\n",
        "        docs_emoji_raw = df_emoji[mask][\"cleaned_content\"].astype(str).tolist()\n",
        "\n",
        "        # Filter: Only keep if the NO-EMOJI text has > 3 words\n",
        "        valid_idx = [i for i, d in enumerate(docs_clean_raw) if len(d.split()) > 3]\n",
        "        docs_clean = [docs_clean_raw[i] for i in valid_idx]\n",
        "        docs_emoji = [docs_emoji_raw[i] for i in valid_idx]\n",
        "\n",
        "        # Embeddings\n",
        "        emb_folder = Path(SAVE_ROOT) / \"embeddings\"\n",
        "        emb_folder.mkdir(parents=True, exist_ok=True)\n",
        "        emb_path = emb_folder / f\"indo-e5_{sentiment}_emoji_context.npy\"\n",
        "\n",
        "        if emb_path.exists():\n",
        "            print(f\"✅ Cached embeddings found: {emb_path}\")\n",
        "            embeddings = np.load(emb_path)\n",
        "        else:\n",
        "            print(f\"Generating embeddings (emoji-word context) for {len(docs_emoji)} docs...\")\n",
        "            embeddings = embed_model.encode(docs_emoji, show_progress_bar=True, batch_size=64, convert_to_numpy=True)\n",
        "            np.save(emb_path, embeddings)\n",
        "            print(f\"✅ Embeddings saved: {emb_path}\")\n",
        "\n",
        "        # Opt.\n",
        "        study_name = f\"{config_name}_{sentiment}_hpo\"\n",
        "        study, optuna_best_model = tuning_per_sentiment2(\n",
        "            docs=docs_clean,\n",
        "            embedding_model=embed_model,\n",
        "            embeddings=embeddings,\n",
        "            vectorizer_model=vectorizer_model,\n",
        "            ctfidf_model=ctfidf_model,\n",
        "            n_trials=n_trials_per_sentiment,\n",
        "            study_name=study_name\n",
        "        )\n",
        "\n",
        "        print(\"\\nBest value (coherence):\", study.best_value)\n",
        "        print(\"Best params:\", study.best_params)\n",
        "\n",
        "        model_folder = Path(base_path) / config_name / sentiment\n",
        "        model_folder.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        hf_model_path = model_folder / \"bertopic\"\n",
        "\n",
        "        optuna_best_model.save(\n",
        "            path=str(hf_model_path),\n",
        "            serialization=\"safetensors\",\n",
        "            save_ctfidf=True,\n",
        "            save_embedding_model=\"LazarusNLP/all-indo-e5-small-v4\"\n",
        "        )\n",
        "\n",
        "        print(f\"✅ BERTopic safetensors model saved to: {hf_model_path}\")\n",
        "\n",
        "        rep_docs = optuna_best_model.get_representative_docs()\n",
        "\n",
        "        rep_docs_serializable = {\n",
        "            str(topic_id): docs\n",
        "            for topic_id, docs in rep_docs.items()\n",
        "        }\n",
        "\n",
        "        rep_docs_path = model_folder / \"representative_docs.json\"\n",
        "\n",
        "        with open(rep_docs_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(rep_docs_serializable, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "        print(f\"✅ Representative docs saved to: {rep_docs_path}\")\n",
        "\n",
        "        # Final metrics\n",
        "        coh = calculate_topic_coherence(optuna_best_model, docs_clean)\n",
        "        div = calculate_topic_diversity(optuna_best_model)\n",
        "\n",
        "        metrics = {\n",
        "            \"c_v\": coh,\n",
        "            \"diversity\": div\n",
        "        }\n",
        "\n",
        "        pd.DataFrame([metrics]).to_csv(model_folder / \"metrics.csv\", index=False)\n",
        "        print(f\"✅ Metrics saved to {model_folder / 'metrics.csv'}\")\n",
        "\n",
        "        save_topics_to_txt(optuna_best_model, model_folder)\n",
        "        create_wordcloud(optuna_best_model, model_folder)\n",
        "\n",
        "        results[sentiment] = {\n",
        "            \"study\": study,\n",
        "            \"topics\": optuna_best_model.get_topics(),\n",
        "            \"metrics\": metrics\n",
        "        }\n",
        "\n",
        "        print(f\"Best Coherence: {coh:.4f}\")\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "24ARBlFMNp8x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RUN"
      ],
      "metadata": {
        "id": "L7Z_UKKgVlsN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "res_config = pipeline(\n",
        "    sentiments=sentiments,\n",
        "    df_emoji=df_emoji_words, # Data with emoji words\n",
        "    df_clean=df_no_emoji,    # Data with no emojis\n",
        "    embed_model=embed_allindo,\n",
        "    vectorizer_model=vectorizer_model,\n",
        "    ctfidf_model=ctfidf_model,\n",
        "    base_path=SAVE_ROOT,\n",
        "    config_name=\"Config\",\n",
        "    n_trials_per_sentiment=50\n",
        ")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Deh2sdrJOI5n"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}